---
title: "lettuce_growth_modeling"
author: "Waldo Ketonou"
date: "2026-02-14"
output: html_document
---
```{r setup, include=TRUE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 8,
  fig.height = 5
)

library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(lubridate)

# Load cleaned dataset
lettuce <- read_csv("data/lettuce_cleaned_for_analysis.csv")

# Convert Date column
lettuce <- lettuce %>%
  mutate(Date = as.Date(Date)) %>%
  drop_na(Growth_Days) # remove rows with missing target
```

```{r feature-selection}
# Select predictors and target
model_data <- lettuce %>%
  select(Growth_Days,
         Temperature, Humidity, TDS_ppm, pH,
         Env_Score,
         Temp_Optimal_num, Humidity_Optimal_num, pH_Optimal_num, TDS_Optimal_num)
```

```{r split-data}
set.seed(123) # for reproducibility

split <- createDataPartition(model_data$Growth_Days, p = 0.8, list = FALSE)
train_data <- model_data[split, ]
test_data  <- model_data[-split, ]
```

```{r linear-model}
lm_model <- lm(Growth_Days ~ ., data = train_data)
summary(lm_model)
```
```{r evaluate-linear}
pred_lm <- predict(lm_model, test_data)

lm_results <- tibble(
  Actual = test_data$Growth_Days,
  Predicted = pred_lm
)

ggplot(lm_results, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Linear Model: Actual vs Predicted Growth Days")

# Metrics
lm_rmse <- sqrt(mean((lm_results$Actual - lm_results$Predicted)^2))
lm_mae  <- mean(abs(lm_results$Actual - lm_results$Predicted))
lm_r2   <- cor(lm_results$Actual, lm_results$Predicted)^2

c(RMSE = lm_rmse, MAE = lm_mae, R2 = lm_r2)
```

```{r random-forest-model, echo=TRUE}
# Build a random forest model to capture nonlinear relationships
# and assess feature importance for predicting Growth_Days

set.seed(123) # for reproducibility

rf_model <- randomForest(
  Growth_Days ~ .,
  data = train_data,
  ntree = 500,
  importance = TRUE
)

# View model summary
print(rf_model)
```

```{r evaluate-rf, echo=TRUE}
# Predict on test set
pred_rf <- predict(rf_model, test_data)

# Combine actual and predicted values
rf_results <- tibble(
  Actual = test_data$Growth_Days,
  Predicted = pred_rf
)

# Plot predicted vs actual
ggplot(rf_results, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "darkorange") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Random Forest: Actual vs Predicted Growth Days")

# Calculate performance metrics
rf_rmse <- sqrt(mean((rf_results$Actual - rf_results$Predicted)^2))
rf_mae  <- mean(abs(rf_results$Actual - rf_results$Predicted))
rf_r2   <- cor(rf_results$Actual, rf_results$Predicted)^2

c(RMSE = rf_rmse, MAE = rf_mae, R2 = rf_r2)
```

```{r evaluate-rf-plot, echo=TRUE}
# Predict on test set
pred_rf <- predict(rf_model, test_data)

# Combine actual and predicted values
rf_results <- tibble(
  Actual = test_data$Growth_Days,
  Predicted = pred_rf
)

# Plot predicted vs actual
ggplot(rf_results, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "darkorange") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Random Forest: Actual vs Predicted Growth Days")

# Calculate performance metrics
rf_rmse <- sqrt(mean((rf_results$Actual - rf_results$Predicted)^2))
rf_mae  <- mean(abs(rf_results$Actual - rf_results$Predicted))
rf_r2   <- cor(rf_results$Actual, rf_results$Predicted)^2

c(RMSE = rf_rmse, MAE = rf_mae, R2 = rf_r2)
```

```{r xgboost-model, echo=TRUE}
# Prepare data for XGBoost (requires matrix format and numeric target)
library(xgboost)

# Convert training and test sets to matrix format
train_matrix <- train_data %>%
  select(-Growth_Days) %>%
  as.matrix()

test_matrix <- test_data %>%
  select(-Growth_Days) %>%
  as.matrix()

# Extract target variable
train_target <- train_data$Growth_Days
test_target  <- test_data$Growth_Days

# Train XGBoost model
set.seed(123)
xgb_model <- xgboost(
  data = train_matrix,
  label = train_target,
  nrounds = 100,
  objective = "reg:squarederror",
  verbose = 0
)
```

```{r evaluate-xgb, echo=TRUE}
# Predict on test set
pred_xgb <- predict(xgb_model, test_matrix)

# Combine actual and predicted values
xgb_results <- tibble(
  Actual = test_target,
  Predicted = pred_xgb
)

# Plot predicted vs actual
ggplot(xgb_results, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "XGBoost: Actual vs Predicted Growth Days")

# Calculate performance metrics
xgb_rmse <- sqrt(mean((xgb_results$Actual - xgb_results$Predicted)^2))
xgb_mae  <- mean(abs(xgb_results$Actual - xgb_results$Predicted))
xgb_r2   <- cor(xgb_results$Actual, xgb_results$Predicted)^2

c(RMSE = xgb_rmse, MAE = xgb_mae, R2 = xgb_r2)
```

```{r model-comparison, echo=TRUE}
# Combine all model metrics into a single tibble
model_comparison <- tibble(
  Model = c("Linear Regression", "Random Forest", "XGBoost"),
  RMSE  = c(lm_rmse, rf_rmse, xgb_rmse),
  MAE   = c(lm_mae,  rf_mae,  xgb_mae),
  R2    = c(lm_r2,   rf_r2,   xgb_r2)
)

model_comparison
```

```{r model-performance-plot, echo=TRUE}
# Reshape for plotting
model_comparison_long <- model_comparison %>%
  pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")

ggplot(model_comparison_long, aes(x = Model, y = Value, fill = Model)) +
  geom_col(position = "dodge") +
  facet_wrap(~Metric, scales = "free_y") +
  theme_minimal() +
  labs(title = "Model Performance Comparison",
       x = "Model",
       y = "Metric Value") +
  scale_fill_viridis_d()
```

### Model Comparison Summary

All three models were evaluated using RMSE, MAE, and R². Random Forest and XGBoost outperform linear regression in predictive accuracy, with XGBoost showing the lowest RMSE and highest R². This suggests that nonlinear relationships and interactions between environmental variables play a significant role in predicting Growth_Days.

**Recommendation:**  
For operational deployment or dashboard integration, XGBoost offers the best performance. However, Random Forest provides strong accuracy with easier interpretability via feature importance. Linear regression remains useful for explaining directional effects and building stakeholder trust.

Next steps may include hyperparameter tuning, cross-validation, and scenario simulation to estimate the impact of environmental improvements.

## Scenario Simulation

We simulate improvements in environmental control to estimate their impact on Growth_Days...

```{r simulate-envscore-boost, echo=TRUE}
# Create a copy of test data
sim_data <- test_data

# Simulate a 10% boost in Env_Score
sim_data$Env_Score <- sim_data$Env_Score * 1.10

# Prepare matrix for prediction
sim_matrix <- sim_data %>% select(-Growth_Days) %>% as.matrix()

# Predict using XGBoost
sim_pred <- predict(xgb_model, sim_matrix)
```

```{r compare-simulation-results, echo=TRUE}
# Compare baseline vs simulated predictions
sim_results <- tibble(
  Actual = test_data$Growth_Days,
  Predicted_Baseline = pred_xgb,
  Predicted_Simulated = sim_pred
)

# Plot comparison
ggplot(sim_results, aes(x = Predicted_Baseline, y = Predicted_Simulated)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Simulated vs Baseline Predictions",
       x = "Baseline Prediction",
       y = "Simulated Prediction")
```
### Simulation Summary

We simulated a 10% improvement in Env_Score across the test set. The model predicts a consistent reduction in Growth_Days, with most plants showing 1–2 day improvements. This validates Env_Score as a meaningful operational lever and supports investment in tighter environmental control.

Future simulations could explore targeted improvements (e.g., pH stabilization) or zone-level interventions to optimize yield and cycle time.